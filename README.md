# Pytorch_Image_Classification_with-Optuna

- For those who are familiar with Pytorch, I would like to present an image classification application on Simpsons dataset, which can be found [here](https://www.kaggle.com/alexattia/the-simpsons-characters-dataset). There are 42 classes in the original data, I will focus on 20 of them in this notebook. The task is to classify these 20 characters. 
- Without going through explonatory data analyis and post-processing, I would like to focus on training. In Part-1, let's build a custom model. In Part-2, I will consider pretrained models.
- For those who are interested only in scores;
- Part1: **overal acc = 89%, mean_recall = 89%, mean_precison = 90%, mean_f1_score = 90%**
- Part2: **overal_acc: 93%, mean_recall = 93%, mean_precison = 92%, mean_f1 = 92%** 
- Hyperparameter optimization is performed with [Optuna](https://optuna.org/) library. It allows us to quickly identify the optimal hyperparamters resulting the best model. Besides from searching the optimal parameters in hyperparamater space, Optuna also provides several early stopping strategies to prune unpromising trials earlier to save the training time. In each notebook, I detailed the training stages clearly.
- One important point is that ,as we will in the notebooks, the dataset is highly imbalanced. There are two mainstream methods to deal with this problem. First one is to oversample the minority class. The idea is to provide almost equal representation to all classses during the training proces. There are several methods in Pytorch to achive this, for example one can use WeightedRandomSampler method. Second method is to provide class weights to the cost function to amplify the error made in low minority classes so that the gradient agressively learns the minority class. 
- I used the second method in this notebook. GetSampleWeights function returns both class weights for the second method as well sample weights for the first method. For oversampling, simply uncomment two lines in GetLoaders function and provide the sampler to train_loader accordingly. 
- Data augmentation is performed via [Albumentation](https://albumentations.ai/) library. You may be used to torchvision but Albumentation is faster and more versatile. 
- In order to speep-up the training, resizing was performed before training. I realized that this step is usually overlooked in many application. However, resizing is a big bottlenect significantly increasing the training time. Call the function below for training and testing set to achieve this.
- One last point, original data is of the form root_dir/classes/images. However, we need something like root_dir/train/classes/images and root_dir/test/classes/images. Creating this structure from the original data is somewhat straighforward. I directly used a piece of code from [Prateek Ramsinghani](https://www.kaggle.com/questions-and-answers/102677) and added line-by-line explanations. If you are lazy, use split-folders library.
